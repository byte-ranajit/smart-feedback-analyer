# ğŸ§  Smart Complaint & Feedback Analyzer (LLM-Powered, FREE & Offline)

A **Spring Bootâ€“based AI system** that analyzes user complaints and feedback to automatically detect **sentiment**, **priority**, **category**, and generate a **polite auto-response** using a **local LLM (LLaMA / Mistral via Ollama)**.

âœ… **100% FREE**  
âœ… **No API key required**  
âœ… **Offline / Local LLM**  
âœ… **Production-style architecture**

---

## ğŸš€ Features

- Accepts customer feedback / complaints via REST API
- AI-powered analysis:
  - Sentiment detection (Positive / Neutral / Negative)
  - Priority classification (LOW / MEDIUM / HIGH)
  - Category detection (Service / Staff / Cleanliness / Other)
  - Auto-generated response
- Uses **local LLM (Ollama)** â€” no OpenAI billing
- Clean layered architecture (Controller â†’ Service â†’ Client â†’ DB)
- Easily switchable to OpenAI / Gemini later

---

## ğŸ—ï¸ Tech Stack

- **Java 17**
- **Spring Boot 3+**
- **Spring Web**
- **Spring Data JPA**
- **H2 Database**
- **Ollama (Local LLM)**
- **LLaMA 3 / Mistral model**
- **REST APIs**

---
## âš™ï¸ Prerequisites

Make sure you have:

- Java 17+
- Maven
- Linux / macOS / Windows (WSL recommended)
- Minimum **8 GB RAM** (for LLaMA models)

---

## ğŸ§  Setup Local LLM (Ollama)

### 1ï¸âƒ£ Install Ollama

```bash
curl -fsSL https://ollama.com/install.sh | sh

Verify:
ollama --version

2ï¸âƒ£ Download a FREE model
Choose one:
ollama pull llama3
or (lighter & faster):
ollama pull mistral

Verify:
ollama list

3ï¸âƒ£ Start Ollama Server
ollama serve
(Ollama runs on http://localhost:11434)

â–¶ï¸ Run the Application
mvn clean spring-boot:run

Spring Boot will start on:
http://localhost:8080

ğŸ”Œ API Usage
Endpoint
POST /api/feedback

Request Body
{
  "userName": "Amit",
  "message": "Room was dirty and staff was rude"
}

Sample Response
{
  "id": 1,
  "userName": "Amit",
  "message": "Room was dirty and staff was rude",
  "sentiment": "Negative",
  "priority": "HIGH",
  "category": "Cleanliness",
  "autoResponse": "We sincerely apologize for your experience. Our team will address this immediately."
}

ğŸ§ª Local Testing (Without LLM Cost)
    No API key required
    Runs completely offline
    Ideal for development, demos, and interviews

ğŸ§  Interview-Ready Explanation

    â€œThis project demonstrates how to integrate LLMs into a Spring Boot backend using a provider-agnostic design. To avoid API costs, I used a local LLaMA model via Ollama, but the architecture allows switching to cloud LLMs in production.â€

ğŸ‘¨â€ğŸ’» Author
Ranajit Khandual
Java & Spring Boot Developer
Focused on backend systems, clean architecture, and AI integration.
